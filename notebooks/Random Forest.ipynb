{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Quick notes\n",
    "---\n",
    "Before we get into the guts of Random Forest Classification, lets get familiar with 2 concepts:\n",
    "- Bootstrapping \n",
    "- Bagging\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "**What is it ?**\n",
    "It is a method of accurately determining a _statistic_. These Statistics are aggregate quantities that cannot be directly observed. For example, the _mean_ of test scores of students in a class. The quantity is _accurately determined_ by minimizing as much as possible. \n",
    "\n",
    "**How do we do it?**\n",
    "Consider a school with a 1000 students. One can easily compute the mean math score of all students $X_{mean}$ with a simple formula.\n",
    "\n",
    "$$X_{mean} = \\frac{\\sum_{i=1}^{1000} x_{i}}{1000}$$\n",
    "\n",
    "This is fantastic. However, computing the mean of a population may not yeild useful results. Lets take a really simple example of this in action. Say we have the test scores of 10 students as follows.\n",
    "\n",
    "```\n",
    "[10, 25, 36, 37, 32, 85, 49, 97, 24, 23]\n",
    "```\n",
    "\n",
    "The mean of this computed from the formula above is $41.8$ . Lets see how different the mean is when bootstrapping. In bootstrapping, we perform the following steps:\n",
    "1. Make $M$ of subsamples from the population (repetition is allowed)\n",
    "2. Calculate the mean of each subsaple clutter. \n",
    "3. Calculate the mean of each of those means. Mean of the means!\n",
    "\n",
    "Considering a population of size $N$ divided into $M$ subsamples with repetition, the general formula for computing the new mean is as follows:\n",
    "\n",
    "$$X_{Bmean} = \\frac{\\sum_{i=1}^{M}\\sum_{j=1}^{len(M_{i})} x_{i,j}}{M}$$\n",
    "\n",
    "Say we create 6 subsamples in the following way:\n",
    "```\n",
    "[10, 36, 32, 49]\n",
    "[25, 32, 97, 10]\n",
    "[36, 85, 24]\n",
    "[97, 23, 24, 25]\n",
    "[85, 10, 36]\n",
    "[23, 32, 24, 25, 36]\n",
    "```\n",
    "The mean for each is $31.75, 41, 48.33, 42.25, 43.66, 28$. Taking $M = 6$ in the equation above, the bootstrapped mean becomes $39.165$. The difference with the original mean $41.8$ may seem trivial. However, this is just a simple example to illustrate bootstrapping. Real applications would involve the construction of hundreds of sample clutters from a population. Bootstrapped quantities better represent the statistic of the population. Greater the number of equally distributed subsamples can yield more accurate statistics.\n",
    "\n",
    "\n",
    "## Bagging\n",
    "\n",
    "**What is it?** It is a method similar to bagging, but applied to model predictions. It works for classification and regression models. The entire training set is broken down into $M$ of sample clutters with repetition. We train $M$ models of the same type using these samples.\n",
    "\n",
    "- While testing **Classification** models, a sample is input to all $M$ models and each will generate an output category. The mode category is considered as the class of the input sample.  For example, in the classification of a human as \"Male\" or \"Female\", say we have trained 10 decision trees. If 7 of them predict an input sample as \"Male\" and the remaining 3 predict \"Female\", then the human sample is labeled \"Male\".\n",
    "\n",
    "- In case of a **Regression** Problem, when we input a sample to all $M$ Regression models, we get $M$ output values. The average of these values is taken as the final predicted value of the sample. For Example, in a House Price Prediction Problem, consider 3 models which predict the price of a house as \\$560,000, \\$580,000, and \\$567,000. The final house price will be the average of these 3 predictions i.e. \\$569,000. \n",
    "\n",
    "Bagging reduces the output variance of the final prediction, despite the fact that the individual models themselves may offer high variance. \n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Using a larger number of estimators will decrease variance of prediction, after which it remains unaffected. Thus, using a large number of estimators will not harm performance; It may just take more time. \n",
    "\n",
    "\n",
    "## Furthur Reading\n",
    "\n",
    "[Bagging and Random Forest Ensemble Algorithms](http://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/)\n",
    "\n",
    "[Ensemble Machine Learning Algorithms with Python](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)\n",
    "\n",
    "[Classification & Regression Trees for Machine Learning](http://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
